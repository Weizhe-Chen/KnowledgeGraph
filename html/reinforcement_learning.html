<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Reinforcement Learning</title>
  <style>
html {
line-height: 1.5;
font-family: Georgia, serif;
font-size: 20px;
color: #1a1a1a;
background-color: #fdfdfd;
}
body {
margin: 0 auto;
max-width: 36em;
padding-left: 50px;
padding-right: 50px;
padding-top: 50px;
padding-bottom: 50px;
hyphens: auto;
overflow-wrap: break-word;
text-rendering: optimizeLegibility;
font-kerning: normal;
}
@media (max-width: 600px) {
body {
font-size: 0.9em;
padding: 1em;
}
h1 {
font-size: 1.8em;
}
}
@media print {
body {
background-color: transparent;
color: black;
font-size: 12pt;
}
p, h2, h3 {
orphans: 3;
widows: 3;
}
h2, h3, h4 {
page-break-after: avoid;
}
}
p {
margin: 1em 0;
}
a {
color: #1a1a1a;
}
a:visited {
color: #1a1a1a;
}
img {
max-width: 100%;
}
h1, h2, h3, h4, h5, h6 {
margin-top: 1.4em;
}
h5, h6 {
font-size: 1em;
font-style: italic;
}
h6 {
font-weight: normal;
}
ol, ul {
padding-left: 1.7em;
margin-top: 1em;
}
li > ol, li > ul {
margin-top: 0;
}
blockquote {
margin: 1em 0 1em 1.7em;
padding-left: 1em;
border-left: 2px solid #e6e6e6;
color: #606060;
}
code {
font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
font-size: 85%;
margin: 0;
}
pre {
margin: 1em 0;
overflow: auto;
}
pre code {
padding: 0;
overflow: visible;
overflow-wrap: normal;
}
.sourceCode {
background-color: transparent;
overflow: visible;
}
hr {
background-color: #1a1a1a;
border: none;
height: 1px;
margin: 1em 0;
}
table {
margin: 1em 0;
border-collapse: collapse;
width: 100%;
overflow-x: auto;
display: block;
font-variant-numeric: lining-nums tabular-nums;
}
table caption {
margin-bottom: 0.75em;
}
tbody {
margin-top: 0.5em;
border-top: 1px solid #1a1a1a;
border-bottom: 1px solid #1a1a1a;
}
th {
border-top: 1px solid #1a1a1a;
padding: 0.25em 0.5em 0.25em 0.5em;
}
td {
padding: 0.125em 0.5em 0.25em 0.5em;
}
header {
margin-bottom: 4em;
text-align: center;
}
#TOC li {
list-style: none;
}
#TOC ul {
padding-left: 1.3em;
}
#TOC > ul {
padding-left: 0;
}
#TOC a:not(:hover) {
text-decoration: none;
}
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
</style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Reinforcement Learning</h1>
</header>
<p>Reinforcement learning (RL) is a type of machine learning that
involves training an agent to make a sequence of decisions in an
environment, with the goal of maximizing a reward. It is often used to
solve problems in which an agent needs to learn how to interact with its
environment in order to achieve a specific goal.</p>
<p>There are several key concepts in RL:</p>
<ul>
<li><strong>Environment</strong>: The environment is the system in which
the agent operates. It can be a physical system (e.g., a robot), a
computer simulation, or a game. The environment provides the agent with
observations and rewards, and the agent can take actions to influence
the environment.</li>
<li><strong>Agent</strong>: The agent is the entity that makes decisions
and takes actions in the environment. It receives observations from the
environment and uses them to select actions that it thinks will maximize
the reward.</li>
<li><strong>Observation</strong>: An observation is a piece of
information about the environment that the agent receives. Observations
can be high-dimensional (e.g., an image) or low-dimensional (e.g., a
scalar).</li>
<li><strong>Action</strong>: An action is a decision made by the agent
based on its observations. The set of possible actions is typically
defined by the environment.</li>
<li><strong>Reward</strong>: A reward is a scalar value that the
environment provides to the agent after it takes an action. The goal of
the agent is to maximize the total reward it receives over time.</li>
</ul>
<p>RL algorithms can be roughly divided into two categories:
<strong>model-based</strong> and <strong>model-free</strong>.
Model-based and model-free algorithms have different strengths and
weaknesses. Model-based algorithms can be more
<strong>efficient</strong> and <strong>faster to converge</strong> than
model-free algorithms, because they can use the model to plan the
optimal action sequence and avoid suboptimal actions. However,
model-based algorithms can also be <strong>more complex and
computationally expensive</strong> than model-free algorithms, because
they need to learn or estimate the model of the environment, which can
be difficult in some cases. Model-free methods can be slower to converge
and less efficient than model-based algorithms, especially in
environments with <strong>long-term dependencies</strong> or
<strong>sparse rewards</strong>.</p>
</body>
</html>
